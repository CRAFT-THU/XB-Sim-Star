/// automatic generate file

#ifndef _STAGE_CONV_${layer_num}
#define _STAGE_CONV_${layer_num}

#include "../${crossbar_file}"
#include "systemc.h"
#include "../ADC.h"
#include "../DAC.h"
#include <omp.h>

using namespace std;

// fifth convolution layer, input channel ${input_channel}, output channel ${output_channel}, pooling ${pooling_size}

// #define POOLING_SIZE ${pooling_size}

SC_MODULE(stage_conv_${layer_num}) {
	sc_in<float> *input;
	sc_out<float> *output;
	sc_in<int> signal_in;
	sc_in<bool> clock_2; // for XB computation
	sc_out<int> signal_out;

	float **pooling_buffer;
	int pooling_pointer;
	int print;

	float *input_buff;
	float *ad_buff;
	bool start;

	// read crossbar data from file
	void init_crossbar() {
		// read from convolution layer ${layer_num}
		float* cell = new float[CROSSBAR_L*CROSSBAR_W];
		string filename = "./weights/weight_${layer_num}.csv";
        {
                FILE * input;
                input = fopen( filename.c_str(), "rb" );
                int ret = fread(cell,sizeof(float ), CROSSBAR_L*CROSSBAR_W ,input);
                if (ret <0 )
                {
                        std::cout << "fread fail\n";
                        return ;
                }
                fclose( input);
        }


		// copy layer ${layer_num} weight matrix to entire_cb weight matrix
/*
		int start_pos = (${layer_num}-1)*CROSSBAR_L*ENTIRE_W+(${layer_num}-1)*CROSSBAR_W;
		for (int i = 0; i < CROSSBAR_L; i++) {
			memcpy(entire_cb.CB_cell+start_pos+i*ENTIRE_W, cell+i*CROSSBAR_W, CROSSBAR_W*sizeof(float));
		}
*/		memcpy( entire_cb.CB_cell[ ${layer_num} ] , cell, CROSSBAR_L*CROSSBAR_W*sizeof(float) );

		delete[] cell;
		cout << "load weights ${layer_num} complete. " << filename << endl;
		// parameters initialize
		pooling_pointer = 0;
		print = 0;
		input_buff = new float[INPUT_SIZE*${input_channel}]();
		ad_buff = new float[CROSSBAR_W]();
		input = new sc_in<float>[INPUT_SIZE*${input_channel}]();
		output = new sc_out<float>[${output_channel}]();
		pooling_buffer = new float*[${output_channel}];
		for (int i = 0; i < ${output_channel}; ++i) {
			pooling_buffer[i] = new float[${image_size}*${pooling_size}]();
		}
		start = false;
	}

	// activation function default relu
	void activation(float tmp[]) {
		for (int i = 0; i < ${output_channel}; i++)
			if (tmp[i] < 0.0)
				tmp[i] = 0.0;
	}

	void add_to_pooling_buffer(float tmp_input[]) {
		for (int i = 0; i < ${output_channel}; i++){
			pooling_buffer[i][pooling_pointer] = tmp_input[i];
		}
	}

	// do maxpooling
	void max_pooling() {
		float tmp_output[${output_channel}];
		int x = pooling_pointer / ${image_size};
		int y = pooling_pointer % ${image_size};

		if ((x % ${pooling_size} == (${pooling_size} - 1)) && (y % ${pooling_size} == (${pooling_size} - 1))) {
			for (int i = 0; i < ${output_channel}; i++) {
				float _max = 0.0; // can not be smaller than 0.0 after relu
				for (int j = 0; j < ${pooling_size}; j++){
					for (int k = 0; k < ${pooling_size}; k++){
						float element = pooling_buffer[i][(x - j) % ${pooling_size}*${image_size} + (y - k)];
						if (element > _max)
							_max = element;
					}
				}
				tmp_output[i] = _max;
			}

			// send to next layer (next buffer)
			for (int i = 0; i < ${output_channel}; i++) {
				output[i].write(tmp_output[i]);
			}
			signal_out.write(signal_in.read());
		}
		pooling_pointer++;
		if (pooling_pointer >= ${image_size} * ${pooling_size})
			pooling_pointer = 0;
	}

	// run convolution
	void stage_conv_run() {

extern int conv_comp_num;
extern int layer_comp_who[LAYER + 1];  //// 15 conv and 4 linear crossbar total 
conv_comp_num ++;
layer_comp_who[ ${layer_num} ] = 1;

	    start = true;

		// with ad & da
		// read data
		memset(input_buff, 0, sizeof(float)*INPUT_SIZE*${input_channel});
		float _max = 0.0;
		for (int i = 0; i < INPUT_SIZE*${input_channel}; ++i){
			input_buff[i] = input[i].read();
			if (input_buff[i] > _max)
				_max = input_buff[i];
		}

		// for movement
		int move = 0;
		for (int i = 0; i < DA_WIDTH; ++i){
			move += int(pow(2, double(i)));
		}

		// scale input
		int n = 0;

		if (${layer_num} == 1)
			n = 8;
		else if (${layer_num} == 2)
			n = 15;
		else n = 14;
		if (n > AD_WIDTH){
			float para = pow(2, AD_WIDTH-n);
			for (int i = 0; i < INPUT_SIZE*${input_channel}; ++i){
				input_buff[i] = int(input_buff[i] * para);
				if (input_buff[i] > 255)
					input_buff[i] = 255;
			}
		}

		// DA->XB->AD
	        float *tmp_input = new float[INPUT_SIZE*${input_channel}]();
		for (int i = 0; i < AD_WIDTH/DA_WIDTH; ++i){
			// lower da_width bits
			for (int j = 0; j < INPUT_SIZE*${input_channel}; ++j){
				int bitnum = static_cast<int>(int(input_buff[j]) & move);
				tmp_input[j] = float(bitnum);
				input_buff[j] = input_buff[j] / pow(2, DA_WIDTH);
			}
/*
	            	memcpy(entire_cb.input+ENTIRE_L*i+(${layer_num}-1)*CROSSBAR_L+CROSSBAR_L-INPUT_SIZE*${input_channel},
                   		tmp_input, sizeof(float)*INPUT_SIZE*${input_channel});
*/			memcpy(entire_cb.input[ ${layer_num} ] + i * CROSSBAR_L +CROSSBAR_L-INPUT_SIZE*${input_channel} ,
				tmp_input, sizeof(float)*INPUT_SIZE*${input_channel});
		}
		delete []tmp_input;
	}

	// read data from crossbar and send them to next buffer layer
    void crossbar_computation(){
	    if (start) {
            ad adc(AD_V);
            memset(ad_buff, 0, sizeof(float)*CROSSBAR_W);
            float *tmp_output = new float[CROSSBAR_W]();
            for (int i = 0; i < AD_WIDTH/DA_WIDTH; ++i) {
/*
                memcpy(tmp_output, entire_cb.output + ENTIRE_W * i + (${layer_num} - 1) * CROSSBAR_W,
                        sizeof(float) * CROSSBAR_W);
*/		memcpy(tmp_output, entire_cb.output[ ${layer_num} ] + i * CROSSBAR_W, sizeof(float) * CROSSBAR_W );
                // ad and shift add
                for (int j = 0; j < CROSSBAR_W; ++j) {
                    float tmp = tmp_output[j] / XB${layer_num}_I;
                    if (tmp > 1)
                        adc.trans(1.0);
                    else if (tmp < -1)
                        adc.trans(-1.0);
                    else
                        adc.trans(tmp);
                    int tmp_ad = int(adc.AD_out);
                    ad_buff[j] = (tmp_ad) * pow(2, i) + ad_buff[j];
                }
            }
            delete []tmp_output;

            activation(ad_buff);
            add_to_pooling_buffer(ad_buff);

            max_pooling(); // pooling size ${pooling_size}
			start = false;
        }
	}

	SC_CTOR(stage_conv_${layer_num}) {
		init_crossbar();

		SC_METHOD(stage_conv_run);
		sensitive << signal_in;
		dont_initialize();

		SC_METHOD(crossbar_computation);
		sensitive << clock_2.pos();
		dont_initialize();
	}

	~stage_conv_${layer_num}() {
//		cb.free_space();
		delete []input_buff;
		delete []ad_buff;
		delete []input;
		delete []output;
		for (int i=0; i<${output_channel}; i++){
			delete []pooling_buffer[i];
		}
		delete pooling_buffer;
	}
};

#endif // !_STAGE_CONV_${layer_num}
